{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ddi_kt_2024.embed.other_embed import sinusoidal_positional_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithPostionOnlyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Only with bert + position encoding\n",
    "    The stucture: [bert_embedding, pos_ent, zero_ent, pos_tag]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                dropout_rate: float = 0.5,\n",
    "                word_embedding_size: int = 768,\n",
    "                position_number: int = 512,\n",
    "                position_embedding_size: int = 128,\n",
    "                position_embedding_type: str = \"normal\",\n",
    "                tag_number: int = 51,\n",
    "                tag_embedding_size: int = 64,\n",
    "                token_embedding_size : int = 256,\n",
    "                conv1_out_channels: int = 256,\n",
    "                conv2_out_channels: int = 256,\n",
    "                conv3_out_channels: int = 256,\n",
    "                conv1_length: int = 1,\n",
    "                conv2_length: int = 2,\n",
    "                conv3_length: int = 3,\n",
    "                target_class: int = 5\n",
    "                ):\n",
    "        super(BertWithPostionOnlyModel, self).__init__()\n",
    "        self.word_embedding_size = word_embedding_size\n",
    "        self.position_embedding_size = position_embedding_size\n",
    "        self.device =\"cuda\"\n",
    "        self.tag_embedding = nn.Embedding(tag_number, tag_embedding_size, padding_idx=0)\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        if position_embedding_type == \"normal\":\n",
    "            self.pos_embedding = nn.Linear(position_number, position_embedding_size, bias=False)\n",
    "        elif position_embedding_type == \"sinusoidal\":\n",
    "            self.pos_embedding = self.sinusoidal_positional_encoding\n",
    "        elif position_embedding_type == \"rotary\":\n",
    "            self.pos_embedding = self.rotary_positional_embedding\n",
    "        else:\n",
    "            raise ValueError(\"Wrong type pos embed\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.normalize_tokens = nn.Linear(in_features = word_embedding_size+tag_embedding_size+position_embedding_size,\n",
    "            out_features=token_embedding_size,\n",
    "            bias=False)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=conv1_out_channels,\n",
    "                      kernel_size=(conv1_length, token_embedding_size),\n",
    "                      stride=1,\n",
    "                      bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=conv2_out_channels,\n",
    "                      kernel_size=(conv2_length, token_embedding_size),\n",
    "                      stride=1,\n",
    "                      bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=conv3_out_channels,\n",
    "                      kernel_size=(conv3_length, token_embedding_size),\n",
    "                      stride=1,\n",
    "                      bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_to_tag = nn.Linear(in_features = conv1_out_channels + conv2_out_channels + conv3_out_channels,out_features=target_class,\n",
    "                        bias=False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def sinusoidal_positional_encoding(self, position):\n",
    "        d_model = int((self.position_embedding_size - 1) / 2)\n",
    "        position = position.unsqueeze(dim=2)\n",
    "        print(position.shape)\n",
    "        angle_rads = torch.arange(d_model) // 2 * torch.pi / torch.pow(10000, 2 * (torch.arange(d_model) // 2) / d_model)\n",
    "        angle_rads = angle_rads.to(self.device)\n",
    "        angle_rads = angle_rads.unsqueeze(dim=0).unsqueeze(dim=0).expand((position.shape[0], 1, angle_rads.shape[0]))\n",
    "        print(position.shape)\n",
    "        print(angle_rads.shape)\n",
    "        angle_rads = torch.bmm(position, angle_rads)\n",
    "        pos_encoding = torch.zeros((angle_rads.shape[0], angle_rads.shape[1], angle_rads.shape[2])).to(self.device)\n",
    "        pos_encoding[:, :, 0::2] = torch.sin(angle_rads[:, :, 0::2])\n",
    "        pos_encoding[:, :, 1::2] = torch.cos(angle_rads[:, :, 1::2])\n",
    "        return pos_encoding\n",
    "\n",
    "    def rotary_positional_embedding(self, position):\n",
    "        d_model = int((self.position_embedding_size - 1) / 2)\n",
    "        position = position.unsqueeze(dim=2)\n",
    "        freqs = torch.exp(torch.linspace(0., -1., int(d_model // 2)+1) * torch.log(torch.tensor(10000.))).to(self.device)\n",
    "        freqs = freqs.unsqueeze(dim=0).unsqueeze(dim=0).expand((position.shape[0], 1, freqs.shape[0]))\n",
    "        angles = position * freqs\n",
    "        rotary_matrix = torch.stack([torch.sin(angles), torch.cos(angles)], axis=-1).to(self.device)\n",
    "        print(rotary_matrix.shape)\n",
    "        return rotary_matrix.reshape((position.shape[0], position.shape[1], d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "\n",
    "        if self.position_embedding_type == \"normal\": # Linear\n",
    "            pos_embedding = self.pos_embedding(x[:,:,self.word_embedding_size: self.word_embedding_size+4])\n",
    "        elif self.position_embedding_type == \"sinusoidal\":\n",
    "            position_embedding_ent = x[:, :, self.word_embedding_size: self.word_embedding_size+4].float()\n",
    "            pos3 = self.sinusoidal_positional_encoding(position_embedding_ent[:, :, 0])\n",
    "            pos4 = self.sinusoidal_positional_encoding(position_embedding_ent[:, :, 1])\n",
    "            pos_embedding = torch.cat((pos3, pos4, position_embedding_ent[:, :, 2:]), dim=2) \n",
    "        else: # rotary\n",
    "            position_embedding_ent = x[:, :, self.word_embedding_size: self.word_embedding_size+4].float()\n",
    "            pos3 = self.rotary_positional_embedding(position_embedding_ent[:, :, 0])\n",
    "            pos4 = self.rotary_positional_embedding(position_embedding_ent[:, :, 1])\n",
    "            pos_embedding = torch.cat((pos3, pos4, position_embedding_ent[:, :, 2:]), dim=2) \n",
    "        print(f\"Re: {pos_embedding.shape}\")\n",
    "        tag_embedding = self.tag_embedding(x[:,:,-1].long())\n",
    "        x = self.normalize_tokens(torch.cat((x[:,:,:self.word_embedding_size], pos_embedding, tag_embedding), dim =2))\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "\n",
    "        x1 = torch.max(x1.squeeze(dim=3), dim=2)[0]\n",
    "        x2 = torch.max(x2.squeeze(dim=3), dim=2)[0]\n",
    "        x3 = torch.max(x3.squeeze(dim=3), dim=2)[0]\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = self.dense_to_tag(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertWithPostionOnlyModel(position_embedding_type='rotary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30, 773])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor = torch.cat((torch.randn(16,30,768), torch.randint(0,30,(16,30,5))), dim=-1)\n",
    "example_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 30, 32, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 30, 63]' is invalid for input of size 30720",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 114\u001b[0m, in \u001b[0;36mBertWithPostionOnlyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# rotary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     position_embedding_ent \u001b[38;5;241m=\u001b[39m x[:, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_size: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m--> 114\u001b[0m     pos3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_positional_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_embedding_ent\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     pos4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_positional_embedding(position_embedding_ent[:, :, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    116\u001b[0m     pos_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((pos3, pos4, position_embedding_ent[:, :, \u001b[38;5;241m2\u001b[39m:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n",
      "Cell \u001b[1;32mIn[47], line 100\u001b[0m, in \u001b[0;36mBertWithPostionOnlyModel.rotary_positional_embedding\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m     98\u001b[0m rotary_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39msin(angles), torch\u001b[38;5;241m.\u001b[39mcos(angles)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(rotary_matrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrotary_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[16, 30, 63]' is invalid for input of size 30720"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "model.forward(example_tensor.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30240"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*30*63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
