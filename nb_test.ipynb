{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from ddi_kt_2024.embed.other_embed import sinusoidal_positional_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/codespace/.python/current/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 100/27792\n",
      "H: 200/27792\n",
      "H: 300/27792\n",
      "H: 400/27792\n",
      "H: 500/27792\n",
      "H: 600/27792\n",
      "H: 700/27792\n",
      "H: 800/27792\n",
      "H: 900/27792\n",
      "H: 1000/27792\n",
      "H: 1100/27792\n",
      "H: 1200/27792\n",
      "H: 1300/27792\n",
      "H: 1400/27792\n",
      "H: 1500/27792\n",
      "H: 1600/27792\n",
      "H: 1700/27792\n",
      "H: 1800/27792\n",
      "H: 1900/27792\n",
      "H: 2000/27792\n",
      "H: 2100/27792\n",
      "H: 2200/27792\n",
      "H: 2300/27792\n",
      "H: 2400/27792\n",
      "H: 2500/27792\n",
      "H: 2600/27792\n",
      "H: 2700/27792\n",
      "H: 2800/27792\n",
      "H: 2900/27792\n",
      "H: 3000/27792\n",
      "H: 3100/27792\n",
      "H: 3200/27792\n",
      "H: 3300/27792\n",
      "H: 3400/27792\n",
      "H: 3500/27792\n",
      "H: 3600/27792\n",
      "H: 3700/27792\n",
      "H: 3800/27792\n",
      "H: 3900/27792\n",
      "H: 4000/27792\n",
      "H: 4100/27792\n",
      "H: 4200/27792\n",
      "H: 4300/27792\n",
      "H: 4400/27792\n",
      "H: 4500/27792\n",
      "H: 4600/27792\n",
      "H: 4700/27792\n",
      "H: 4800/27792\n",
      "H: 4900/27792\n",
      "H: 5000/27792\n",
      "H: 5100/27792\n",
      "H: 5200/27792\n",
      "H: 5300/27792\n",
      "H: 5400/27792\n",
      "H: 5500/27792\n",
      "H: 5600/27792\n",
      "H: 5700/27792\n",
      "H: 5800/27792\n",
      "H: 5900/27792\n",
      "H: 6000/27792\n",
      "H: 6100/27792\n",
      "H: 6200/27792\n",
      "H: 6300/27792\n",
      "H: 6400/27792\n",
      "H: 6500/27792\n",
      "H: 6600/27792\n",
      "H: 6700/27792\n",
      "H: 6800/27792\n",
      "H: 6900/27792\n",
      "H: 7000/27792\n",
      "H: 7100/27792\n",
      "H: 7200/27792\n",
      "H: 7300/27792\n",
      "H: 7400/27792\n",
      "H: 7500/27792\n",
      "H: 7600/27792\n",
      "H: 7700/27792\n",
      "H: 7800/27792\n",
      "H: 7900/27792\n",
      "H: 8000/27792\n",
      "H: 8100/27792\n",
      "H: 8200/27792\n",
      "H: 8300/27792\n",
      "H: 8400/27792\n",
      "H: 8500/27792\n",
      "H: 8600/27792\n",
      "H: 8700/27792\n",
      "H: 8800/27792\n",
      "H: 8900/27792\n",
      "H: 9000/27792\n",
      "H: 9100/27792\n",
      "H: 9200/27792\n",
      "H: 9300/27792\n",
      "H: 9400/27792\n",
      "H: 9500/27792\n",
      "H: 9600/27792\n",
      "H: 9700/27792\n",
      "H: 9800/27792\n",
      "H: 9900/27792\n",
      "H: 10000/27792\n",
      "H: 10100/27792\n",
      "H: 10200/27792\n",
      "H: 10300/27792\n",
      "H: 10400/27792\n",
      "H: 10500/27792\n",
      "H: 10600/27792\n",
      "H: 10700/27792\n",
      "H: 10800/27792\n",
      "H: 10900/27792\n",
      "H: 11000/27792\n",
      "H: 11100/27792\n",
      "H: 11200/27792\n",
      "H: 11300/27792\n",
      "H: 11400/27792\n",
      "H: 11500/27792\n",
      "H: 11600/27792\n",
      "H: 11700/27792\n",
      "H: 11800/27792\n",
      "H: 11900/27792\n",
      "H: 12000/27792\n",
      "H: 12100/27792\n",
      "H: 12200/27792\n",
      "H: 12300/27792\n",
      "H: 12400/27792\n",
      "H: 12500/27792\n",
      "H: 12600/27792\n",
      "H: 12700/27792\n",
      "H: 12800/27792\n",
      "H: 12900/27792\n",
      "H: 13000/27792\n",
      "H: 13100/27792\n",
      "H: 13200/27792\n",
      "H: 13300/27792\n",
      "H: 13400/27792\n",
      "H: 13500/27792\n",
      "H: 13600/27792\n",
      "H: 13700/27792\n",
      "H: 13800/27792\n",
      "H: 13900/27792\n",
      "H: 14000/27792\n",
      "H: 14100/27792\n",
      "H: 14200/27792\n",
      "H: 14300/27792\n",
      "H: 14400/27792\n",
      "H: 14500/27792\n",
      "H: 14600/27792\n",
      "H: 14700/27792\n",
      "H: 14800/27792\n",
      "H: 14900/27792\n",
      "H: 15000/27792\n",
      "H: 15100/27792\n",
      "H: 15200/27792\n",
      "H: 15300/27792\n",
      "H: 15400/27792\n",
      "H: 15500/27792\n",
      "H: 15600/27792\n",
      "H: 15700/27792\n",
      "H: 15800/27792\n",
      "H: 15900/27792\n",
      "H: 16000/27792\n",
      "H: 16100/27792\n",
      "H: 16200/27792\n",
      "H: 16300/27792\n",
      "H: 16400/27792\n",
      "H: 16500/27792\n",
      "H: 16600/27792\n",
      "H: 16700/27792\n",
      "H: 16800/27792\n",
      "H: 16900/27792\n",
      "H: 17000/27792\n",
      "H: 17100/27792\n",
      "H: 17200/27792\n",
      "H: 17300/27792\n",
      "H: 17400/27792\n",
      "H: 17500/27792\n",
      "H: 17600/27792\n",
      "H: 17700/27792\n",
      "H: 17800/27792\n",
      "H: 17900/27792\n",
      "H: 18000/27792\n",
      "H: 18100/27792\n",
      "H: 18200/27792\n",
      "H: 18300/27792\n",
      "H: 18400/27792\n",
      "H: 18500/27792\n",
      "H: 18600/27792\n",
      "H: 18700/27792\n",
      "H: 18800/27792\n",
      "H: 18900/27792\n",
      "H: 19000/27792\n",
      "H: 19100/27792\n",
      "H: 19200/27792\n",
      "H: 19300/27792\n",
      "H: 19400/27792\n",
      "H: 19500/27792\n",
      "H: 19600/27792\n",
      "H: 19700/27792\n",
      "H: 19800/27792\n",
      "H: 19900/27792\n",
      "H: 20000/27792\n",
      "H: 20100/27792\n",
      "H: 20200/27792\n",
      "H: 20300/27792\n",
      "H: 20400/27792\n",
      "H: 20500/27792\n",
      "H: 20600/27792\n",
      "H: 20700/27792\n",
      "H: 20800/27792\n",
      "H: 20900/27792\n",
      "H: 21000/27792\n",
      "H: 21100/27792\n",
      "H: 21200/27792\n",
      "H: 21300/27792\n",
      "H: 21400/27792\n",
      "H: 21500/27792\n",
      "H: 21600/27792\n",
      "H: 21700/27792\n",
      "H: 21800/27792\n",
      "H: 21900/27792\n",
      "H: 22000/27792\n",
      "H: 22100/27792\n",
      "H: 22200/27792\n",
      "H: 22300/27792\n",
      "H: 22400/27792\n",
      "H: 22500/27792\n",
      "H: 22600/27792\n",
      "H: 22700/27792\n",
      "H: 22800/27792\n",
      "H: 22900/27792\n",
      "H: 23000/27792\n",
      "H: 23100/27792\n",
      "H: 23200/27792\n",
      "H: 23300/27792\n",
      "H: 23400/27792\n",
      "H: 23500/27792\n",
      "H: 23600/27792\n",
      "H: 23700/27792\n",
      "H: 23800/27792\n",
      "H: 23900/27792\n",
      "H: 24000/27792\n",
      "H: 24100/27792\n",
      "H: 24200/27792\n",
      "H: 24300/27792\n",
      "H: 24400/27792\n",
      "H: 24500/27792\n",
      "H: 24600/27792\n",
      "H: 24700/27792\n",
      "H: 24800/27792\n",
      "H: 24900/27792\n",
      "H: 25000/27792\n",
      "H: 25100/27792\n",
      "H: 25200/27792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 03:48:55,685 - DEBUG - Starting new HTTPS connection (1): huggingface.co:443\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 25300/27792\n",
      "H: 25400/27792\n",
      "H: 25500/27792\n",
      "H: 25600/27792\n",
      "H: 25700/27792\n",
      "H: 25800/27792\n",
      "H: 25900/27792\n",
      "H: 26000/27792\n",
      "H: 26100/27792\n",
      "H: 26200/27792\n",
      "H: 26300/27792\n",
      "H: 26400/27792\n",
      "H: 26500/27792\n",
      "H: 26600/27792\n",
      "H: 26700/27792\n",
      "H: 26800/27792\n",
      "H: 26900/27792\n",
      "H: 27000/27792\n",
      "H: 27100/27792\n",
      "H: 27200/27792\n",
      "H: 27300/27792\n",
      "H: 27400/27792\n",
      "H: 27500/27792\n",
      "H: 27600/27792\n",
      "H: 27700/27792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 03:48:56,034 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "2024-03-26 03:48:56,299 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-26 03:48:56,301 - DEBUG - Attempting to acquire lock 140614910765344 on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/e7e5a29ad0904cb11c014ce669b56d07c19997cf.lock\n",
      "2024-03-26 03:48:56,302 - DEBUG - Lock 140614910765344 acquired on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/e7e5a29ad0904cb11c014ce669b56d07c19997cf.lock\n",
      "2024-03-26 03:48:56,565 - DEBUG - https://huggingface.co:443 \"GET /allenai/scibert_scivocab_uncased/resolve/main/config.json HTTP/1.1\" 200 385\n",
      "2024-03-26 03:48:56,574 - DEBUG - Attempting to release lock 140614910765344 on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/e7e5a29ad0904cb11c014ce669b56d07c19997cf.lock\n",
      "2024-03-26 03:48:56,575 - DEBUG - Lock 140614910765344 released on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/e7e5a29ad0904cb11c014ce669b56d07c19997cf.lock\n",
      "2024-03-26 03:48:56,956 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "2024-03-26 03:48:57,212 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "2024-03-26 03:48:57,214 - DEBUG - Attempting to acquire lock 140614910768800 on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/8a2c6673dbaca55493a7449f60f0bf6c18a7e107.lock\n",
      "2024-03-26 03:48:57,215 - DEBUG - Lock 140614910768800 acquired on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/8a2c6673dbaca55493a7449f60f0bf6c18a7e107.lock\n",
      "2024-03-26 03:48:57,489 - DEBUG - https://huggingface.co:443 \"GET /allenai/scibert_scivocab_uncased/resolve/main/vocab.txt HTTP/1.1\" 200 227845\n",
      "2024-03-26 03:48:57,980 - DEBUG - Attempting to release lock 140614910768800 on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/8a2c6673dbaca55493a7449f60f0bf6c18a7e107.lock\n",
      "2024-03-26 03:48:57,981 - DEBUG - Lock 140614910768800 released on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/8a2c6673dbaca55493a7449f60f0bf6c18a7e107.lock\n",
      "2024-03-26 03:48:58,310 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/tokenizer.json HTTP/1.1\" 404 0\n",
      "2024-03-26 03:48:58,574 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/added_tokens.json HTTP/1.1\" 404 0\n",
      "2024-03-26 03:48:58,843 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/special_tokens_map.json HTTP/1.1\" 404 0\n",
      "2024-03-26 03:48:59,279 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-26 03:48:59,538 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-26 03:48:59,807 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/model.safetensors HTTP/1.1\" 404 0\n",
      "2024-03-26 03:49:00,080 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/model.safetensors.index.json HTTP/1.1\" 404 0\n",
      "2024-03-26 03:49:00,347 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/pytorch_model.bin HTTP/1.1\" 302 0\n",
      "2024-03-26 03:49:00,360 - DEBUG - Attempting to acquire lock 140614910770144 on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/e492944d88ac97dee6baa547671d3c526a3d067676883efb058311f4e5882e1a.lock\n",
      "2024-03-26 03:49:00,361 - DEBUG - Lock 140614910770144 acquired on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/e492944d88ac97dee6baa547671d3c526a3d067676883efb058311f4e5882e1a.lock\n",
      "2024-03-26 03:49:00,382 - DEBUG - Starting new HTTPS connection (1): cdn-lfs.huggingface.co:443\n",
      "2024-03-26 03:49:00,514 - DEBUG - https://cdn-lfs.huggingface.co:443 \"GET /allenai/scibert_scivocab_uncased/e492944d88ac97dee6baa547671d3c526a3d067676883efb058311f4e5882e1a?response-content-disposition=attachment%3B+filename*%3DUTF-8%27%27pytorch_model.bin%3B+filename%3D%22pytorch_model.bin%22%3B&response-content-type=application%2Foctet-stream&Expires=1711684140&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxMTY4NDE0MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9hbGxlbmFpL3NjaWJlcnRfc2Npdm9jYWJfdW5jYXNlZC9lNDkyOTQ0ZDg4YWM5N2RlZTZiYWE1NDc2NzFkM2M1MjZhM2QwNjc2NzY4ODNlZmIwNTgzMTFmNGU1ODgyZTFhP3Jlc3BvbnNlLWNvbnRlbnQtZGlzcG9zaXRpb249KiZyZXNwb25zZS1jb250ZW50LXR5cGU9KiJ9XX0_&Signature=bTLnzCTc2yDulPEy~7nMRw2wAsNQ9C5S8YG2Sy6Yn95CgYOPyl8YWKc1r9ApFT1Ob8d1fWbxGBd3KvbGl4aEI77flf7BWc9GdPfvJt8qvRPSAC~J4SWU2uKdIGP~CTGov~4k81SiBfPHh8MkA7sx1mK7EWMJfmb9fTIgICLoAQhD428EBiaJMx~rtFhQFXNdJW1WmWl4Ks6~p~CCjzKfym4a6vASF328Igsx~JaO3lGlNqsNfl7iGcHRUGJxOsDdkohtNSGwr4GEEYNW0f8gQ1GJBnkwIWia0ka1~3~dPRaaEIVp-hXXmAYT9q5nlnXpuHo4It1u5sVD4hjYTqtbnQ__&Key-Pair-Id=KVTP0A1DKRTAX HTTP/1.1\" 200 442221694\n",
      "2024-03-26 03:49:02,614 - DEBUG - Attempting to release lock 140614910770144 on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/e492944d88ac97dee6baa547671d3c526a3d067676883efb058311f4e5882e1a.lock\n",
      "2024-03-26 03:49:02,618 - DEBUG - Lock 140614910770144 released on /home/codespace/.cache/huggingface/hub/.locks/models--allenai--scibert_scivocab_uncased/e492944d88ac97dee6baa547671d3c526a3d067676883efb058311f4e5882e1a.lock\n",
      "/home/codespace/.python/current/lib/python3.10/site-packages/transformers/data/processors/glue.py:66: FutureWarning: This function will be removed from the library soon, preprocessing should be handled with the 🤗 Datasets library. You can have a look at this example script for pointers: https://github.com/huggingface/transformers/blob/main/examples/pytorch/text-classification/run_glue.py\n",
      "  warnings.warn(DEPRECATION_WARNING.format(\"function\"), FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "H: 100/5716\n",
      "H: 200/5716\n",
      "H: 300/5716\n",
      "H: 400/5716\n",
      "H: 500/5716\n",
      "H: 600/5716\n",
      "H: 700/5716\n",
      "H: 800/5716\n",
      "H: 900/5716\n",
      "H: 1000/5716\n",
      "H: 1100/5716\n",
      "H: 1200/5716\n",
      "H: 1300/5716\n",
      "H: 1400/5716\n",
      "H: 1500/5716\n",
      "H: 1600/5716\n",
      "H: 1700/5716\n",
      "H: 1800/5716\n",
      "H: 1900/5716\n",
      "H: 2000/5716\n",
      "H: 2100/5716\n",
      "H: 2200/5716\n",
      "H: 2300/5716\n",
      "H: 2400/5716\n",
      "H: 2500/5716\n",
      "H: 2600/5716\n",
      "H: 2700/5716\n",
      "H: 2800/5716\n",
      "H: 2900/5716\n",
      "H: 3000/5716\n",
      "H: 3100/5716\n",
      "H: 3200/5716\n",
      "H: 3300/5716\n",
      "H: 3400/5716\n",
      "H: 3500/5716\n",
      "H: 3600/5716\n",
      "H: 3700/5716\n",
      "H: 3800/5716\n",
      "H: 3900/5716\n",
      "H: 4000/5716\n",
      "H: 4100/5716\n",
      "H: 4200/5716\n",
      "H: 4300/5716\n",
      "H: 4400/5716\n",
      "H: 4500/5716\n",
      "H: 4600/5716\n",
      "H: 4700/5716\n",
      "H: 4800/5716\n",
      "H: 4900/5716\n",
      "H: 5000/5716\n",
      "H: 5100/5716\n",
      "H: 5200/5716\n",
      "H: 5300/5716\n",
      "H: 5400/5716\n",
      "H: 5500/5716\n",
      "H: 5600/5716\n",
      "H: 5700/5716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-26 03:49:18,533 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "2024-03-26 03:49:18,802 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-26 03:49:19,072 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/tokenizer_config.json HTTP/1.1\" 404 0\n",
      "2024-03-26 03:49:19,357 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/vocab.txt HTTP/1.1\" 200 0\n",
      "2024-03-26 03:49:19,655 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/config.json HTTP/1.1\" 200 0\n",
      "2024-03-26 03:49:19,921 - DEBUG - https://huggingface.co:443 \"HEAD /allenai/scibert_scivocab_uncased/resolve/main/config.json HTTP/1.1\" 200 0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch.utils.data.dataset.TensorDataset at 0x7fe378427190>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, BertModel\n",
    "from transformers.data.processors.utils import InputExample, InputFeatures, DataProcessor\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from ddi_kt_2024.utils import *\n",
    "from ddi_kt_2024.preprocess.asada_preprocess import *\n",
    "\n",
    "candidates = load_pkl(\"cache/pkl/v2/notprocessed.candidates.train.pkl\")\n",
    "examples = convert_to_examples(candidates, \"train\", data_type=\"ddi\", mask=True)\n",
    "preprocess(examples, model_name=\"allenai/scibert_scivocab_uncased\", max_seq_length=256, save_path=\"ddi_train.pt\")\n",
    "\n",
    "candidates = load_pkl(\"cache/pkl/v2/notprocessed.candidates.test.pkl\")\n",
    "examples = convert_to_examples(candidates, \"test\", data_type=\"ddi\", mask=True)\n",
    "preprocess(examples, model_name=\"allenai/scibert_scivocab_uncased\", max_seq_length=256, save_path=\"ddi_test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertWithPostionOnlyModel(nn.Module):\n",
    "    \"\"\"\n",
    "    Only with bert + position encoding\n",
    "    The stucture: [bert_embedding, pos_ent, zero_ent, pos_tag]\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                dropout_rate: float = 0.5,\n",
    "                word_embedding_size: int = 768,\n",
    "                position_number: int = 512,\n",
    "                position_embedding_size: int = 128,\n",
    "                position_embedding_type: str = \"normal\",\n",
    "                tag_number: int = 51,\n",
    "                tag_embedding_size: int = 64,\n",
    "                token_embedding_size : int = 256,\n",
    "                conv1_out_channels: int = 256,\n",
    "                conv2_out_channels: int = 256,\n",
    "                conv3_out_channels: int = 256,\n",
    "                conv1_length: int = 1,\n",
    "                conv2_length: int = 2,\n",
    "                conv3_length: int = 3,\n",
    "                target_class: int = 5\n",
    "                ):\n",
    "        super(BertWithPostionOnlyModel, self).__init__()\n",
    "        self.word_embedding_size = word_embedding_size\n",
    "        self.position_embedding_size = position_embedding_size\n",
    "        self.device =\"cuda\"\n",
    "        self.tag_embedding = nn.Embedding(tag_number, tag_embedding_size, padding_idx=0)\n",
    "        self.position_embedding_type = position_embedding_type\n",
    "        if position_embedding_type == \"normal\":\n",
    "            self.pos_embedding = nn.Linear(position_number, position_embedding_size, bias=False)\n",
    "        elif position_embedding_type == \"sinusoidal\":\n",
    "            self.pos_embedding = self.sinusoidal_positional_encoding\n",
    "        elif position_embedding_type == \"rotary\":\n",
    "            self.pos_embedding = self.rotary_positional_embedding\n",
    "        else:\n",
    "            raise ValueError(\"Wrong type pos embed\")\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "        self.normalize_tokens = nn.Linear(in_features = word_embedding_size+tag_embedding_size+position_embedding_size,\n",
    "            out_features=token_embedding_size,\n",
    "            bias=False)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=conv1_out_channels,\n",
    "                      kernel_size=(conv1_length, token_embedding_size),\n",
    "                      stride=1,\n",
    "                      bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=conv2_out_channels,\n",
    "                      kernel_size=(conv2_length, token_embedding_size),\n",
    "                      stride=1,\n",
    "                      bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=1,\n",
    "                      out_channels=conv3_out_channels,\n",
    "                      kernel_size=(conv3_length, token_embedding_size),\n",
    "                      stride=1,\n",
    "                      bias=False),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.dense_to_tag = nn.Linear(in_features = conv1_out_channels + conv2_out_channels + conv3_out_channels,out_features=target_class,\n",
    "                        bias=False)\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def sinusoidal_positional_encoding(self, position):\n",
    "        d_model = int((self.position_embedding_size - 1) / 2)\n",
    "        position = position.unsqueeze(dim=2)\n",
    "        print(position.shape)\n",
    "        angle_rads = torch.arange(d_model) // 2 * torch.pi / torch.pow(10000, 2 * (torch.arange(d_model) // 2) / d_model)\n",
    "        angle_rads = angle_rads.to(self.device)\n",
    "        angle_rads = angle_rads.unsqueeze(dim=0).unsqueeze(dim=0).expand((position.shape[0], 1, angle_rads.shape[0]))\n",
    "        print(position.shape)\n",
    "        print(angle_rads.shape)\n",
    "        angle_rads = torch.bmm(position, angle_rads)\n",
    "        pos_encoding = torch.zeros((angle_rads.shape[0], angle_rads.shape[1], angle_rads.shape[2])).to(self.device)\n",
    "        pos_encoding[:, :, 0::2] = torch.sin(angle_rads[:, :, 0::2])\n",
    "        pos_encoding[:, :, 1::2] = torch.cos(angle_rads[:, :, 1::2])\n",
    "        return pos_encoding\n",
    "\n",
    "    def rotary_positional_embedding(self, position):\n",
    "        d_model = int((self.position_embedding_size - 1) / 2)\n",
    "        position = position.unsqueeze(dim=2)\n",
    "        freqs = torch.exp(torch.linspace(0., -1., int(d_model // 2)+1) * torch.log(torch.tensor(10000.))).to(self.device)\n",
    "        freqs = freqs.unsqueeze(dim=0).unsqueeze(dim=0).expand((position.shape[0], 1, freqs.shape[0]))\n",
    "        angles = position * freqs\n",
    "        rotary_matrix = torch.stack([torch.sin(angles), torch.cos(angles)], axis=-1).to(self.device)\n",
    "        print(rotary_matrix.shape)\n",
    "        return rotary_matrix.reshape((position.shape[0], position.shape[1], d_model))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.float()\n",
    "\n",
    "        if self.position_embedding_type == \"normal\": # Linear\n",
    "            pos_embedding = self.pos_embedding(x[:,:,self.word_embedding_size: self.word_embedding_size+4])\n",
    "        elif self.position_embedding_type == \"sinusoidal\":\n",
    "            position_embedding_ent = x[:, :, self.word_embedding_size: self.word_embedding_size+4].float()\n",
    "            pos3 = self.sinusoidal_positional_encoding(position_embedding_ent[:, :, 0])\n",
    "            pos4 = self.sinusoidal_positional_encoding(position_embedding_ent[:, :, 1])\n",
    "            pos_embedding = torch.cat((pos3, pos4, position_embedding_ent[:, :, 2:]), dim=2) \n",
    "        else: # rotary\n",
    "            position_embedding_ent = x[:, :, self.word_embedding_size: self.word_embedding_size+4].float()\n",
    "            pos3 = self.rotary_positional_embedding(position_embedding_ent[:, :, 0])\n",
    "            pos4 = self.rotary_positional_embedding(position_embedding_ent[:, :, 1])\n",
    "            pos_embedding = torch.cat((pos3, pos4, position_embedding_ent[:, :, 2:]), dim=2) \n",
    "        print(f\"Re: {pos_embedding.shape}\")\n",
    "        tag_embedding = self.tag_embedding(x[:,:,-1].long())\n",
    "        x = self.normalize_tokens(torch.cat((x[:,:,:self.word_embedding_size], pos_embedding, tag_embedding), dim =2))\n",
    "        \n",
    "        x = x.unsqueeze(1)\n",
    "\n",
    "        x1 = self.conv1(x)\n",
    "        x2 = self.conv2(x)\n",
    "        x3 = self.conv3(x)\n",
    "\n",
    "        x1 = torch.max(x1.squeeze(dim=3), dim=2)[0]\n",
    "        x2 = torch.max(x2.squeeze(dim=3), dim=2)[0]\n",
    "        x3 = torch.max(x3.squeeze(dim=3), dim=2)[0]\n",
    "        \n",
    "        x = torch.cat((x1, x2, x3), dim=1)\n",
    "        x = self.dense_to_tag(x)\n",
    "        x = self.softmax(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BertWithPostionOnlyModel(position_embedding_type='rotary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 30, 773])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_tensor = torch.cat((torch.randn(16,30,768), torch.randint(0,30,(16,30,5))), dim=-1)\n",
    "example_tensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 30, 32, 2])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "shape '[16, 30, 63]' is invalid for input of size 30720",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m model\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mforward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexample_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcuda\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[47], line 114\u001b[0m, in \u001b[0;36mBertWithPostionOnlyModel.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m: \u001b[38;5;66;03m# rotary\u001b[39;00m\n\u001b[0;32m    113\u001b[0m     position_embedding_ent \u001b[38;5;241m=\u001b[39m x[:, :, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_size: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mword_embedding_size\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m4\u001b[39m]\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[1;32m--> 114\u001b[0m     pos3 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrotary_positional_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition_embedding_ent\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    115\u001b[0m     pos4 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrotary_positional_embedding(position_embedding_ent[:, :, \u001b[38;5;241m1\u001b[39m])\n\u001b[0;32m    116\u001b[0m     pos_embedding \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((pos3, pos4, position_embedding_ent[:, :, \u001b[38;5;241m2\u001b[39m:]), dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m) \n",
      "Cell \u001b[1;32mIn[47], line 100\u001b[0m, in \u001b[0;36mBertWithPostionOnlyModel.rotary_positional_embedding\u001b[1;34m(self, position)\u001b[0m\n\u001b[0;32m     98\u001b[0m rotary_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstack([torch\u001b[38;5;241m.\u001b[39msin(angles), torch\u001b[38;5;241m.\u001b[39mcos(angles)], axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28mprint\u001b[39m(rotary_matrix\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m--> 100\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrotary_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m\u001b[43m(\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mposition\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43md_model\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: shape '[16, 30, 63]' is invalid for input of size 30720"
     ]
    }
   ],
   "source": [
    "model.to('cuda')\n",
    "model.forward(example_tensor.to('cuda'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30240"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "16*30*63"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multimodal_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
