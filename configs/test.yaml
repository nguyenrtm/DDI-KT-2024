
# PLEASE CREATE NEW CONFIG INSTEAD OF OVERRIDE THIS CONFIG
# Change this string to change the folder name of saving models
training_session_name: "Test_training"

# Wandb init info
representation: "SDP"
preprocess: "None"
features: "fasttext, tag, position"

# Lookup
type_embed: 'bert_pos_unpad'
lookup_word: 'cache/fasttext/nguyennb/all_words.txt'
lookup_tag: 'cache/fasttext/nguyennb/all_pos.txt'
lookup_dep: 'cache/fasttext/nguyennb/all_dep.txt'
lookup_direction: 'cache/fasttext/nguyennb/all_direction.txt'

# Load pkl config
all_candidates_train: 'cache/pkl/v2/notprocessed.candidates.train.pkl'
all_candidates_test: 'cache/pkl/v2/notprocessed.candidates.test.pkl'
sdp_train_mapped: 'cache/pkl/v2/notprocessed.mapped.sdp.train.pkl'
sdp_test_mapped: 'cache/pkl/v2/notprocessed.mapped.sdp.test.pkl'
fasttext_path: 'cache/fasttext/nguyennb/fastText_ddi.npz'
vocab_path: 'cache/fasttext/nguyennb/all_words.txt'

# Saved customdataset
train_custom_dataset: 'D:\ResearchMultimodal\DDI-KT-2024\bc5_scibert_scivocab_uncased_train.pt'
test_custom_dataset: 'D:\ResearchMultimodal\DDI-KT-2024\bc5_scibert_scivocab_uncased_test.pt'

# Train configs
min_batch_size: 9
batch_size: 64
weight_decay: 0.001
lr: 0.0001
epochs: 300
dropout_rate: 0
word_embedding_size: 768
tag_number: 51
tag_embedding_size: 50
position_number: 4
position_embedding_size: 50
position_embedding_type: 'normal'
direction_number: 3
direction_embedding_size: 50
edge_number: 46
edge_embedding_size: 200
token_embedding_size: 500
dep_embedding_size: 500
conv1_out_channels: 256
conv2_out_channels: 256
conv3_out_channels: 256
conv1_length: 3
conv2_length: 6
conv3_length: 9
w_false: 27792 / 23771
w_advice: 27792 / 826
w_effect: 27792 / 1687
w_mechanism: 27792 / 1319
w_int: 27792 / 189
device: "cuda"
parallel_training: False
target_class: 5

